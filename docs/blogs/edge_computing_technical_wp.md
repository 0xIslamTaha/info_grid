##  Myths

### Hyper scale data centers are by design more efficient
There are advantages in building large scale datacenters like  economies of scale and shear bulk buying power  compared to small ones but these are not as big as people think. The average cost per rack in a hyperscale datacenter is 20-35K USD including all energy & safety systems. The cost of the hardware per rack is around the  200-300k USD mark.    

What is often forgotten is that anything that is of enormous scale and therefore highly concentrated or dense brings complexity and a specific set of problems to deal with. This adds resource requirements like investment, operational costs, knowledge and people.    

This is often ignored or waved aside because solving complexities like this have become an industry in itself.  IT is in peoples nature to solve challenges and develop new way to deal with challenges.  But let's use those resources to solve real problems, not self invented challenges.     

In reality keeping things simple & small is much more cost effective than a big complex environment.  So creating a hyperscale datacenter presents a very interesting challenge and there are specialist companies to help build and solve problems along the way but it is not the most cost effective way to deliver datacenter services.    

### Big data centers can be made green and this is super important
If you throw in all assets needed to build a big data center - land, real estate and all required hardware, power and staff- it leaves a huge carbon footprint.   Power a datacenter with wind, hydro or solar power and use technology to improve the PUE (Power Usage Effectiveness) of a datacenter helps to reduce the overall carbon footprint.  This is the strategy that most datacenter operators have adopted to decrease their carbon footprint and write about this in their corporate social responsibility.    

A 20% reduction in the PUE factor of a datacenter sounds like big deal but it's still an improvement in the overhead power consumption.  Improvements are mostly sought and found in the actual cooling technology of a datacenter or by sourcing zero carbon footprint power sources.  What is hardly ever looked it is the actual power consumed by the equipment that runs the IT workloads inside these datacenter.  Actual servers, storage chassis, physical disks, switch farms and tape libraries.    

These devices make up 100% it the real carbon footprint of a datacenter while the PUE only speaks about the overhead power consumption to cool the facility, to open and close the doors of the facility and the security systems that keep the bad people out and the good people in.    

The real improvement lies in deploying technologies that actually consume less power to deliver the actual IT capacity to run the workloads, real CPU chassis, physical disks and storage cabinets. Working on how hardware is being more effectively used can have an impact of 1000% and lead to 10 times more power efficiency. More info see this [blog](http://www.threefoldtoken.com/tech/2017/5/27/10x-times-power-savings-is-this-possible)  

### Redundant systems have a better uptime?
This is how all of us (of age ;-) have been raised.  Systems need redundancy mechanisms to improve their operational uptime and reliability.  And to be honest if you look at it for a minutes this seems to make a lot of sense….  if you are looking at it through IT tainted glasses.  But translate this to the non IT World: Simple example:      

In order to make your car more reliable we add redundancy as we do in IT.  So for the risk of having a puncture we add one extra tyre for all the tyres we use continuously.  This adds 4 extra tyres to the car.  Then a decision needs to be made:  Do we put those tyres in a structure where they are always running along the primary tyres or do we chose not to have them "online" all the time and wearing and tearing as the primary tyres?      
To have them online continuously is inefficient and wasteful and therefore we need to built a very complicated structure that puts the tyres on the road the second (more accurate microsecond) that the car detects a tyre puncture.  To build this system which adds high availability to a car is definitely an engineering challenge that would keep a (large) number of engineers (electronic, hydraulic, mechanical and computer) busy for awhile and I am sure that all of them would come up with a working solution.  But…..    

This solution would add a lot of complexity, would change the the look and feel of a car completely, would add a lot of unnecessary petrol to overcome the extra drag and so and so on.  Also would you feel comfortable to run this car down the motorway at 100 miles an hour and have to risk of it putting down four extra tyres?  I don't think so and in reality this has not been done for mainstream transportation - for certain speciality vehicles we have invented tyres that cannot deflated, thereby solving the issue in the simplest possible manner.  Solve the root cause.    

The IT industry has gone mental with the concept of everything needs to be built and expanded to create redundancy, it forgot somehow to look at root causes which helped spawning a whole new industry of itself, that obviously want to keep themselves in the business of creating complicated and expensive redundant system.  A self fulfilling prophecy  But has it actually made IT system more reliable?    

### Big companies with a certain track record will know better how to optimize, they have more people…

At first sight this sounds logical but still if we look at the IT landscape today 90%+ of innovation in IT is done by small startups. The big legacy IT companies have a huge heritage they hardly can overcome. They are locked in old infrastructure designs and after a while the build out of the infrastructure becomes the brake for their business. Real innovation dies a slows death and in comes the pain killer approach.    

When companies come of size red tape enters the structure and innovation comes to a halt.  Maintaining the status quo operationally, building brand, improving the bottom line are the new tactical goals that are redefined every financial year.  Innovation and R&D budgets shrink and the technology marginally improves to be able to have annual announcements.    

Obviously this is a very negative view of large established companies, but to a degree we all recognise the difficulties to achieve real change in such large entreprises.  On the contrary small startup of even scale up companies are agile, have an overall     

The innovation to overcome these hurdles is mostly blocked internally by the established power houses. Startups on the other hand are more keen to develop new things and have no history to deal with.    


## Some Crazy Facts

### Big majority of users in the world connect to servers not being present in their region and as such experience higher cost & low performance
<img class="mdexporter-image" src="assets/image_0.jpeg" alt="" height="281" width="601">
According to a study of [datacentermap.com](http://www.datacentermap.com) 80% of the datacenters of internet service providers are based in the US and Europe.  The rest of the world has scarce resources dotted around territories.    

As a consequence most internet users use internet based services running on infrastructure which is located far away from their physical point of presence and most likely outside their country borders. This decreases the general end user experience (latency) but also adds unnecessary costs for actually transporting information back and forth and for enterprises it adds legislation and compliance headaches.     

On a humanitarian level the opportunities for creativity, learning and development of new generations is influenced in a negative way by not having access to good, affordable internet services keeping the status quo between the developed and less developed world.  The United Nations have declared internet access a global [human right](http://www.businessinsider.com/un-says-internet-access-is-a-human-right-2016-7?international=true&r=US&IR=T). See UN resolution [here.](null)  

### Compute & Storage vendors create poor performing solutions.
We have enjoyed year on year improvements of hardware following [Moore's law.](https://en.wikipedia.org/wiki/Moore%27s_law)  While this has allowed us to progress and innovate it has also lead to software developers taking these advances for granted and cutting corners where they could creating sub optimals code and allowing software components to be layered on top of each other to achieve certain functionality of behavior.    

The inefficiencies have now lead us to be in a situation where organically grown IT architecture are immensely complex and use a variety of components from different soft and hardware vendors integrates by a so called integrator. The overall effort and cost involved to create, operate and maintain such architectures is growing continuously and requiring an ever increasing budget and resourcing to continue.    

If we could go down to the core algorithms and take another looks at these, innovate at the heart of technology instead of applying patches and pain killers we would be able to create a lot more end user capacity that what systems provide today.  The result of this is that systems will last longer and do not have to be replaced by faster ones, it will take a lot less engineers to create, operate and maintain these systems and overall they will present a more stable and reliable platform achieving higher levels of uptime.      

Only upsides, right?  Well a huge downside of such an approach is that vendors will make less revenue and more importantly less margin as system will run for longer, more stable and requires less updates.  Why would vendors innote at the core of their solution……

### Ten times more power efficiency can in e.g. storage systems.
Today global Internet infrastructure requires enormous amounts of energy - well north of the entire annual electricity consumption of the United Kingdom - ranking among the more pollutive industries globally (similar to airlines!)    
We believe IT can do a lot better - in fact we believe we can reduce the Internet’s carbon footprint by 10 times compared to other industry standard IT capacity producing solutions.    
Power consumption is a function of better compute and storage performance requiring more racks and more cooling. Our solutions achieve roughly 3 times the performance per rack (so we use fewer racks) - and our racks require less energy than typical racks in the industry :-)    
Read more at this blog: [10x times power savings, is this possible?](http://www.threefoldtoken.com/tech/2017/5/27/10x-times-power-savings-is-this-possible)  

### Biggest cost of running IT architectures are people.
Today's complex, built out of ‘band aid and patches’ point solutions, organically grown and badly documented IT infrastructures need an armada of people to keep it ticking.  Find an example IT budget [here](http://www.gartner.com/downloads/public/explore/metricsAndTools/ITBudget_Sample_2012.pdf).  Even though this is an example budget one can learn a lot from the trend that are presented in the example:    
- On average an IT budget takes 5% of overall revenues
- IT consumes 6.5% of the total number of FTE in the company of which 85% are insource and 15% is on payroll.  This means that the enterprise doesn't retain internal know how how to operate their IT.
- Just under 50% of the IT budget is spent on Infrastructure and operations and a similar amount is spent on Applications. A mere 5% is considered to be internal overhead within the IT department.
- Around 65% of the IT budget is spend on resources and services, around 35% is spend on Hard and Software.

These figures presents industry average number but they still paint a troubling picture that the cost of IT are a sizeable part of an overall budget and most spend is going to have the right knowledge skills insourced to the organisation to run the core IT architecture on which the whole company operations rely -  for any other discipline in any organisation this would present an unacceptable risk to the business and it's continuity - strangely not for IT.    

### Biggest source of downtime in computer systems are people
Getting people involved in fixing infrastructure problems creates the risk of accidentally  causing more system downtime. A very recent example on this hit a large organisation providing cloud services: [https://aws.amazon.com/message/41926/](https://aws.amazon.com/message/41926/)  

20+ years ago when internet datacenters came into existence next to telecom points of presence (POP'’) the level of complication in architecting, building and maintaining these infrastructures exploded.  From an already reasonable complicated technology setup to transport packets of data around the globe these information warehouses were created where data was uploaded to, processed and the obtained results send back to end users often many hundreds of kilometers away.    

Managing a datacenter that contains solutions for transporting information, for storing information and to process information is not an easy feat and the growth of data volume uploaded, stored and processed has exponentially increased. The number of technologies invented and implemented processing and storing this information has also exploded  which as a combination results in a double exponential growth in complexity to architect, build operate and maintain these.    

The time has come that we cannot rely on people anymore to do the right thing in case of emergency - the complexity is overwhelming and the dependency on this information being available is huge.  Manual deployments and operational responsibility by people is not providing the agility and speed to keep up with the continuous exponential growth of the industry.  It is time to take the human element out of Information Technology and let smart systems take over.    

### The current growth of internet needs 4000+ new big data centers of computer systems
The global data growth will reach more than 40 Zetabytes by 2020 which represents an average year over year growth from 42% starting at 4.5 Zetabytes in 2013*. To host all this data over 4000 new big data centers need to be built. To achieve this goal, $ trillion USD investment capital are needed and land totalling the size of the UK would be needed.     

[https://www.siemens.com/innovation/en/home/innovation-strategy/driving-forward-digitalization.html](https://www.siemens.com/innovation/en/home/innovation-strategy/driving-forward-digitalization.html)  


## Our Solution

### Bring workloads closer to people.
We all produce more infromation / data every day and we process this data more intensely every day.  The fact that we collect and create data in one location and have to transport it to another for processing not not scaling for much longer.  The world needs a solution where data is processed as close as possible to it's origin.  Our mission is to bring IT workloads as close as possible to the actual people requiring -the service. To achieve this goal data centers need to be literally cut into thousands of small IT capacity generators which are distributed as close as possible the user - to the edge of the network.    

### Use hardware better
Hardware has become amazingly powerful over the last few decades but few technologists know how to use the capabilities in the most efficient way. On the other hand many IT soft and hardware vendors are not interested to develop better algorithms as these would undermine their current revenues and profits. So most IT vendors chose to update components in the products and solutions and never overhaul any core principles or architecture / algorithms.   Customers need to buy bigger machines - compute and storage- to compensate to deliver capacity for the increased capacity needs and it they want to use tracks with skillsets which are very difficult to find)    

Some examples    
- Hardware like infiniband with RDMA (Remote Direct Memory Access) has been available for more than 15 years but still none of the technology vendors have adopted this technology. RDMA improves latencies 100 times and avoids CPU intensive context switches but goes against the sales of a very elaborate, complicated and expensive network switching farm.
- New network cards have baked in hardware instruction sets which can be used to offload a lot of standard overhead tasks but these are still not used in mainstream solutions.
- CPU’s are amazingly powerful but lots instruction sets are not used.

Most of the above is reality because operating systems & development environments have limited, sometimes no support for these advanced features.    

Real specialists and long time experience is required to create software which efficiently harnesses the power present in modern day hardware. Literally 10 to 100 times benefits can be achieved for certain workloads if and when the available performance in hardware is used to fuel these workloads in an effective manner.    

### Use different algorithms to resolve problems in a different way
Anything which happens in computers is executed by using algorithms but many of these algorithms are the result of old thinking paradigms which today are no longer optimal. Still people do not change these algorithms because there is business and margin impact in doing so, it is also very hard to do, requires a brave and bold management team to support it and serious IT specialists with decades of experience are required.    

E.g. our storage algorithms allow us without the implementation of compression to use 3 times less disks and still achieving 10 times better reliability. We have been working on these algorithms since early 2000. Multiple companies which we have been creating in this space have been acquired for their expertise in this domain.    

### Less = More
The overhead of different layers in the IT industry is a result of technology vendors inventing new and profitable business models. Complexity is a business - selling maintenance services, 2 hour response SLA's and integration professionals to make complex end solutions work together.    

This has been made possible by the tremendous performance improvements achieved by hardware and component vendors. The increased performance of devices and components have created a market where complicated integrations were made possible, where additional layers integrating new and old IT architectures prevented both technology vendors and users to reconsider how to effectively use and improve existing architectures.     

Examples of this complication leading to more hard and software are:    
- Going from physical to virtual allowed for hardware to be used by more end users and end user workloads - the sharing principle.  However in order to virtualise hardware an additional layer was introduced between the end user application and the base OS: the hypervisor. In order to maintain and operate a large number of virtualisation platforms and an even larger number of virtual machines / end user workloads like OSS and BSS platforms were invented to do these tasks effectively.  It created a complete new industry providing installation, operation and maintenance services.
- More recent we have seen an uprising of containerised services.  This movement supports the less is more principle by chopping large monolithic IT workloads into small microservices that can be run anywhere - supporting the trend of IoT data collection and processing everywhere. However many of these container are not run in an efficient way as most of the computed capacity today present in (public) datacenters is purposed to run IaaS (virtual machines) which required the container to be run in a virtual machine, adding another layer of virtualisation and creating complexity and overhead.
- Tiered storage is still a defacto standard in datacenters: online, nearline, offline (archive) living in silos, run on and by different technology and algorithms. But there is no actual need but to support different technologies and technology vendors to continue their (successful) business model.  When designed correctly a general purpose storage system should be able to provide performant, distributed, reliable storage but also must be able to cater for arching workloads (plan / book less capacity for this particular storage workload). Using standard off the shelf components, HDD's, SSD's, SATA controllers and some CPU to run a software defined storage solution that allows for any workloads to run effectively.
- More recent we have seen an uprising of containerised services.  This movement supports the less is more principle by chopping large monolithic IT workloads into small microservices that can be run anywhere - supporting the trend of IoT data collection and processing everywhere. However many of these container are not run in an efficient way as most of the computed capacity today present in (public) datacenters is purposed to run IaaS (virtual machines) which required the container to be run in a virtual machine, adding another layer of virtualisation and creating complexity and overhead.
- Tiered storage is still a defacto standard in datacenters: online, nearline, offline (archive) living in silos, run on and by different technology and algorithms. But there is no actual need but to support different technologies and technology vendors to continue their (successful) business model.  When designed correctly a general purpose storage system should be able to provide performant, distributed, reliable storage but also must be able to cater for arching workloads (plan / book less capacity for this particular storage workload). Using standard off the shelf components, HDD's, SSD's, SATA controllers and some CPU to run a software defined storage solution that allows for any workloads to run effectively.

We do not believe in such an approach. We believe that the answer is to avoid complexity, to not want to build a business model out of supporting and maintaining IT solutions.  To create IT solution that work, just like electricity is coming to our business and home without us worrying too much about all of the complicated things that need to be done in order to generate electricity - IT should be a commodity that is available for use case and that require very little knowledge and integration skills.    

### A new operating system called Zero-OS
We have created a new operating system which eliminates most of the layers and gives some dramatic benefits.    
- No install required, because there is nothing to install
- No updates required, because there are no local files to upgrade, there is no local state in the operating system
- A shell does not exist which means there is no environment for hackers to live in
- No local filesystem, every file is fingerprinted and only served when required.

Our operating system is called Zero-OS. Learn more in our Zero-OS blog    


## How to get the benefits of this technology

### Use our TF Grid.
Each TF Node is running GIG open source technology    

### Usage of GIG open source software
GIG believes that core components of all good system software needs to be open source. This guarantees better reliability & security, because anyone using the software knows exactly what's inside and a bigger community can and will help to improve the software.    

Most of the core parts of the GIG technology are open source (point to our open source blog).    

Please note that no service or any guarantee can be given on the open source software, GIG also has a commercial license for each of these components which allow people to get the required support & quality assurance.    

### Buy the technology from GIG...
GIG has created this technology over the last 15 years and takes a disruptive approach through elimination, simplification and using tech innovations. For more information see [www.gig.tech](www.gig.tech)  
